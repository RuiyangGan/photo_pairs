# preprocess features to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.04
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
max(table(rslt_large$cluster)[-1])
# preprocess features to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.04
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
max(table(rslt_large$cluster)[-1])
View(dat_large.features)
View(dat_large.features)
# preprocess features to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
max(table(rslt_large$cluster)[-1])
# preprocess features to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
max(table(rslt_large$cluster)[-1])
# preprocess features to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = 0.03, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = 0.1, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.05, minPts=2)
kable(table(rslt_large$cluster)[-1])
max(table(rslt_large$cluster)[-1])
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, eps = 0.05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
library(knitr)
library(dbscan)
knitr::opts_chunk$set(echo = TRUE)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .03, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .03, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
View(dat_large)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# Group galaxies with similar redshifts
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, eps = .05, minPts=2)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features,eps = .05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large$redshift, dat_large.features.mag)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
View(rslt_2000)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
rslt_2000
plot(rslt_2000$simplified_tree)
dbscan(dat_2000$redshift, eps = 0.02, minPts=2)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
plot(mod1, what = "classification")
par(mfrow = c(2,2))
plot(mod1, what = "uncertainty", dimens = c(2,1), main = "")
plot(mod1, what = "uncertainty", dimens = c(3,1), main = "")
plot(mod1, what = "uncertainty", dimens = c(2,3), main = "")
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
mod1
mod1$classification
table(mod1$classification)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(AIC)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
AIC <- mclustAIC(photo.pairs_sub)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
EM <- mclustEM(photo.pairs_sub)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
mod1
typeof(mod1$classification)
class(mod1$classification)
str(mod1$classification)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, minPts=2, eps = .05)
rslt_2000
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
# Read in the large scale data
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
View(dat_large)
tinytex::install_tinytex()
tinytex::install_tinytex(force=TRUE)
