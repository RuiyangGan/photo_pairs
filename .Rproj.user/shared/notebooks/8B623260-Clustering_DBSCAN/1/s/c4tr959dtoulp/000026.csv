"0","# preprocess features columns to be selected"
"0","dat_large <- read.csv(""Photo_pairs_50000.csv"", header = TRUE, skip=1)"
"0",""
"0","dat_large.features.cmag <- dat_large[c(""c_g"",""c_r"",""c_i"",""c_z"")]/dat_large$u"
"0","dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c(""deVAB_g"",""deVAB_r"",""deVAB_i"",""deVAB_z"")]/dat_large$deVAB_u"
"0","#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c(""expAB_g"",""expAB_r"",""expAB_i"",""expAB_z"")]/dat_large$expAB_u"
"0","dat_large.features <- data.frame(dat_large$redshift, dat_large.features.mag)"
"0",""
"0",""
"0","# Normalize feature matrix to have the same sample variance"
"0","dat_large.features <- scale(dat_large.features, scale=TRUE)"
"0",""
"0","# I try different radius of the neighborhood and found out .05"
"0","# to be a good parameter as almost all groups have "
"0","rslt_large <- hdbscan(dat_large.features, minPts=2)"
"2","Error: cannot allocate vector of size 9.3 Gb
"
