dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = 0.1, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.03, minPts=2)
kable(table(rslt_large$cluster)[-1])
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features, eps = 0.05, minPts=2)
kable(table(rslt_large$cluster)[-1])
max(table(rslt_large$cluster)[-1])
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, eps = 0.05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
library(knitr)
library(dbscan)
knitr::opts_chunk$set(echo = TRUE)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.03
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .03, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .03, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.mag <- dat_large[c("g","r","i","z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
View(dat_large)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.deVAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u,dat_2000$redshift)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large.features.expAB, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$c_u
#dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- dbscan(dat_large.features,eps = .05, minPts=2)
rslt_large
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# Group galaxies with similar redshifts
dat_2000.features <- data.frame(dat_2000[,c("g","r","i","z")]/dat_2000$u)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, eps = .05, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, eps = .05, minPts=2)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2)
rslt_2000
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features,eps = .05, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large.features.mag, dat_large$redshift)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
# preprocess features columns to be selected
dat_large <- read.csv("Photo_pairs_50000.csv", header = TRUE, skip=1)
dat_large.features.cmag <- dat_large[c("c_g","c_r","c_i","c_z")]/dat_large$u
dat_large.features.deVAB <- dat_large[,colnames(dat_large) %in% c("deVAB_g","deVAB_r","deVAB_i","deVAB_z")]/dat_large$deVAB_u
#dat_large.features.expAB <- dat_large[,colnames(dat_large) %in% c("expAB_g","expAB_r","expAB_i","expAB_z")]/dat_large$expAB_u
dat_large.features <- data.frame(dat_large$redshift, dat_large.features.mag)
# Normalize feature matrix to have the same sample variance
dat_large.features <- scale(dat_large.features, scale=TRUE)
# I try different radius of the neighborhood and found out .05
# to be a good parameter as almost all groups have
rslt_large <- hdbscan(dat_large.features, minPts=2)
View(rslt_2000)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
rslt_2000
plot(rslt_2000$simplified_tree)
dbscan(dat_2000$redshift, eps = 0.02, minPts=2)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
plot(mod1, what = "classification")
par(mfrow = c(2,2))
plot(mod1, what = "uncertainty", dimens = c(2,1), main = "")
plot(mod1, what = "uncertainty", dimens = c(3,1), main = "")
plot(mod1, what = "uncertainty", dimens = c(2,3), main = "")
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
mod1
mod1$classification
table(mod1$classification)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(AIC)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
AIC <- mclustAIC(photo.pairs_sub)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
EM <- mclustEM(photo.pairs_sub)
photo.pairs = read.csv("Photo_pairs.csv")
names(photo.pairs)
photo.pairs_sub = subset(photo.pairs, select=c("redshift"))
library(mclust)
BIC <- mclustBIC(photo.pairs_sub)
plot(BIC)
mod1 <- Mclust(photo.pairs_sub, x = BIC)
mod1
typeof(mod1$classification)
class(mod1$classification)
str(mod1$classification)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- hdbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
rslt_2000
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, minPts=2, gen_simplified_tree=TRUE)
# read the data
dat_2000 <- read.csv("Photo_pairs.csv", header = TRUE)
dat_2000.features <- data.frame(dat_2000$redshift, dat_2000[,c("g","r","i","z")]/dat_2000$u)
# Normalize data matrix to have the same variance
dat_2000.features <- scale(dat_2000.features, scale=TRUE)
# I try different radius of the neighborhood and found out 0.05
# to be a good parameter as almost all groups have
rslt_2000 <- dbscan(dat_2000.features, minPts=2, eps = .05)
rslt_2000
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
# Read in the large scale data
dat_large <- read.csv("Photo_pairs_100000.csv", header = TRUE, skip=1)
View(dat_large)
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
# Load the csv file into data and
dat_small <- read.csv("Photo_pairs.csv", header = TRUE, skip = 1)
dat_small <- dat_small[-which(dat_small$redshift>1.2),]
library(dbscan)
view(dat_small)
View(dat_small)
pairs(dat_small[,c("g,"r","i","z")]/dat_small$u)
pairs(dat_small[,c("g,"r","i","z")]/dat_small$u)
pairs(dat_small[,c(g,"r","i","z")])
pairs(dat_small[,c("g","r","i","z")])
pairs(dat_small[,c("g,"r","i","z")]/dat_small$c_u)
pairs(dat_small[,c("c_g","c_r","c_i","c_z")]/dat_small$c_u)
pairs(scale(dat_small[,c("c_g","c_r","c_i","c_z")]/dat_small$c_u))
pairs(scale(dat_small[,c("g","r","i","z")]/dat_small$u))
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
# Load the csv file into data and
dat_small <- read.csv("Photo_pairs.csv", header = TRUE, skip = 1)
dat_small <- dat_small[-which(dat_small$redshift>1.2),]
library(dbscan)
# Load the csv file into data and
dat_small <- read.csv("Photo_pairs.csv", header = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dbscan)
View(dat_small)
dat_small$objid
dat_small$objid[1]
dat_small$objid[1] == dat_small$objid
dat_small$objid[1] == dat_small$objid[2]
dat_small$objid[1] == dat_small$objid[1]
# Read in small data
dat_small <- read.csv("Photo_pairs.csv", header = TRUE, skip = 1)
# get rid of anomalies
anom_idx <- dat_small$objid[dat_small$c_u < -10]
min(dat_small$u)
min(dat_small$c_u)
knitr::opts_chunk$set(echo = TRUE)
library(dbscan)
# Read in small scaled data
dat_small <- read.csv("Photo_pairs.csv", header = TRUE, skip = 1)
range(dat_small$redshift)
View(dat_small)
?seq
seq(dat_small)
seq(dat_small$redshift)
seq(dat_small$redshift, length.out=10)
# Use rolling window to obtain subset of galaxies with similar redshift
# Use window centers evenly spread out through the range of
redshift_centers <- seq(from = min(dat_small$redshift),
to = max(dat_small$redshift),
length.out=11)
# Read in small scaled data
dat_small <- read.csv("Photo_pairs.csv", header = TRUE, skip = 1)
redshift <- dat_small$redshift
# Use rolling window to obtain subset of galaxies with similar redshift
# Use window centers evenly spread out through the range of
redshift_centers <- seq(from = min(dat_small$redshift),
to = max(dat_small$redshift),
length.out=11)
# Use normalized composite magnitude
cMag_bands <- c("c_u", "c_g", "c_r", "c_i", "c_z")
cMag_norm <- dat_small[, cMag_bands]/dat_small$c_u
# Use a bandwidth of 0.5 to select galaxy with similar red shifts
# at each window center
h <- 0.5
for(c in redshift_centers) {
dat_sub <- dat_small[]
<- dat_
# Use rolling window to obtain subset of galaxies with similar redshift
# Use window centers evenly spread out through the range of
redshift_centers <- seq(from = min(dat_small$redshift),
to = max(dat_small$redshift),
length.out=11)
# Use normalized composite magnitude
cMag_bands <- c("c_u", "c_g", "c_r", "c_i", "c_z")
cMag_norm <- dat_small[, cMag_bands]/dat_small$c_u
# Use a bandwidth of 0.5 to select galaxy with similar red shifts
# at each window center
h <- 0.5
for(c in redshift_centers) {
dat_sub <- dat_small[abs(redshift-c) <= h/2,]
}
View(dat_sub)
?dbscna
?dbscan
?DBSCAN
